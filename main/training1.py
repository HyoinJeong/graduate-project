# -*- coding: utf-8 -*-
"""Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Axy-BGMHp_mqS_CB_Q8ifzsiobAfyTfH
"""

import os
import glob
import shutil
import librosa
import soundfile as sf
import sklearn
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import random

import tensorflow as tf
import keras
import torch
from keras import layers, models ,Model ,Input
from keras.layers import Dense, Flatten, Activation, BatchNormalization
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, KFold
from tensorflow.keras.utils import to_categorical
from keras.callbacks import EarlyStopping

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

Train_Dir = '/content/gdrive/MyDrive/VoiceRecognition/data/Train/'
Test_Dir = '/content/gdrive/MyDrive/VoiceRecognition/data/Test/'

def MFCC(audio_path):
  path = audio_path
  sample_rate = 16000

  x = librosa.load(path,sample_rate)[0]
  S = librosa.feature.melspectrogram(x, sr=sample_rate, n_mels=128)
  log_S = librosa.power_to_db(S, ref=np.max)
  mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=20)

  delta2_mfcc = librosa.feature.delta(mfcc, order=2)

  return mfcc, delta2_mfcc

def padding(list,*array) :
 max = 0
 for i in range(len(list)):
    if max == 0 or list[i].shape[1] > max :
      max = list[i].shape[1]
 for i in range(len(list)):
   addnum = max - list[i].shape[1]
   list[i] = np.pad(list[i], [(0, 0), (0, addnum)], mode='constant')
 for i in array:
   for j in range(len(list)):
     if str(type(i)) == "<class 'numpy.ndarray'>" :
       addnum = i[0].shape[1] - list[j].shape[1]
       if addnum >= 0:
         list[j] = np.pad(list[j], [(0, 0), (0, addnum)], mode='constant')
       else :
         list[j] = list[j][:,0:i[0].shape[1]]
     elif str(type(i)) == "<class 'keras.engine.keras_tensor.KerasTensor'>" :
       addnum = i.shape[2] - list[j].shape[1]
       if addnum >= 0:
         list[j] = np.pad(list[j], [(0, 0), (0, addnum)], mode='constant')
       else :
         list[j] = list[j][:,0:i.shape[2]]
     elif str(type(i)) == "<class 'tuple'>" :
       addnum = i[1] - list[j].shape[1]
       if addnum >= 0:
         list[j] = np.pad(list[j], [(0, 0), (0, addnum)], mode='constant')
       else :
         list[j] = list[j][:,0:i[1]]

def fulldataLearn(newDataDir):
  data = []
  target = []
  data2 = []
  target2 = []
  Train_Dir = '/content/gdrive/MyDrive/VoiceRecognition/data/Train/'
  
  for i, (root, dirs, files) in enumerate (os.walk(Train_Dir)):
    for file in files :
      if '.wav'  not in file in file :
        continue
      else :
        audio_path = os.path.join(root,file)
        mfcc, delta = MFCC(audio_path)
        mfcc = mfcc.astype(float)
        dirname = os.path.dirname(audio_path) 
        data.append(mfcc)
        target.append(dirname)

  for i, (root, dirs, files) in enumerate (os.walk(newDataDir)):
      for file in files :
        if '.wav'  not in file in file :
          continue
        else :
          audio_path = os.path.join(root,file)
          mfcc, delta = MFCC(audio_path)
          mfcc = mfcc.astype(float)
          dirname = os.path.dirname(audio_path) 
          data.append(mfcc)
          target.append(dirname)
  
  padding(data)
  data = np.array(data)

  encoder = LabelEncoder()
  encoder.fit(target)
  target_encoded = encoder.transform(target)

  target = tf.keras.utils.to_categorical(np.array(target_encoded))

  print(data.shape)
  print(target.shape)

  train_x, val_x, train_y, val_y = train_test_split(data, target, test_size=0.4, shuffle=True, stratify=target, random_state=42)
  test_x, val_x, test_y, val_y = train_test_split(val_x, val_y, random_state=42, test_size=0.5, stratify=val_y)

  train_x = np.expand_dims(train_x, -1)
  test_x = np.expand_dims(test_x, -1)
  val_x = np.expand_dims(val_x, -1)

  model = CNN(train_x)
  net = Dense(target.shape[1], activation='softmax' ,name="dense_a")(model.layers[-2].output)
  model = Model(inputs = model.input, outputs = net)

  model.summary()
  model.compile(loss='categorical_crossentropy',
                optimizer='adam',
                metrics=['accuracy'])

  model.fit(train_x,
            train_y,
            epochs=50,
            batch_size=32,
            verbose=1,
            validation_data=(val_x, val_y))

  return model

class Data_augmentation:
  def __init__(self, filepath, num):
    self.filepath = filepath
    self.num = num
  def whiteNoise(self, signal, noise_rate):
    noise = np.random.normal(0, signal.std(), signal.size)
    augmented_signal = signal + noise * noise_rate
    return augmented_signal
  def timeStretch(self, signal, stretch_rate):
    return librosa.effects.time_stretch(signal, stretch_rate)
  def pitchScaling(self, signal, sr, pitch_step):
    return librosa.effects.pitch_shift(signal, sr, pitch_step)
  def reverse(self, signal):
    return signal * -1

def CNN(train_x):   
    input = Input(shape=train_x[0].shape)
    
    hidden1 = layers.Conv2D(32, (3, 3), padding='same',strides = (2,2),activation='relu')(input)

    hidden2 = layers.Conv2D(32*2, (3, 3), padding='same',strides = (2,2), activation='relu')(hidden1)
    
    hidden3 = layers.Conv2D(32*3, (3, 3), padding='same',strides = (2,2),activation='relu')(hidden2)

    hidden4 = layers.Conv2D(32*4, (3, 3), padding='same',strides = (2,2),activation='relu')(hidden3)

    hidden5 = layers.Conv2D(32*5, (3, 3), padding='same',strides = (2,2),activation='relu')(hidden4)

    hidden = Flatten()(hidden5)
    hidden = layers.BatchNormalization()(hidden)
    hidden = Dense(32, activation='relu')(hidden)

    output = Dense(20, activation='softmax')(hidden)

    model = Model(input, output)

    return model

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score

def get_eval(test_y, pred):
  confusion = confusion_matrix(test_y, pred)
  accuracy = accuracy_score(test_y, pred)
  pre_macro = precision_score(test_y, pred, average='macro')
  pre_micro = precision_score(test_y, pred, average='micro')
  recall_macro = recall_score(test_y, pred, average='macro')
  recall_micro = recall_score(test_y, pred, average='micro')
  f1macro = f1_score(test_y, pred, average='macro')
  f1micro = f1_score(test_y, pred, average='micro')

  print('Confusion Matrix')
  print(confusion)
  print('Accuracy:{}' .format(accuracy))
  print('Precision(Macro):{}, Recall(Macro):{}'.format(pre_macro, recall_macro))
  print('Precision(Micro):{}, Recall(Micro):{}'.format(pre_micro, recall_micro))
  print('F1-Score(Macro) : {}, F1-Score(Micro) : {}' .format(f1macro, f1micro))
